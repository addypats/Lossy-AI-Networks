batch_size: 2
dataset: piqa
eval_steps: 100
fp16: false
learning_rate: 3.0e-05
local_rank: 0
loss_rate: 0.01
max_length: 128
max_samples: 0
max_steps: 100000
model_name: gpt2-medium
output_dir: output_gpt2-medium/tp_gpt2-medium_precision-fp32_Num_Nodes-2_Data-piqa_lr0.01_batch_size_8
patience: 3
seed: 1234
target_accuracy: 0.85
tensor_parallel_size: 2
weight_decay: 0.01
