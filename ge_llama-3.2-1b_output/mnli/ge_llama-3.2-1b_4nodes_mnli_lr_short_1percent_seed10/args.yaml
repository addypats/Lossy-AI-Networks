batch_size: 32
dataset: mnli
epochs: 4
eval_steps: 20
fp16: false
ge_config: short_1percent
learning_rate: 2.0e-05
logging_steps: 10
loss_rate: 0.001
loss_type: g-e
max_length: 256
max_samples: 0
model_name: meta-llama/Llama-3.2-1B
num_nodes: 4
num_unfrozen_layers: null
output_dir: ge_llama-3.2-1b_output/mnli
run_id: ge_llama-3.2-1b_4nodes_mnli_lr_short_1percent_seed10
save_steps: 100
seed: 10
