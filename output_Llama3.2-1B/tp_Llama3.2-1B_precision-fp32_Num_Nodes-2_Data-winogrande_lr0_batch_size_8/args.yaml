batch_size: 8
dataset: winogrande
eval_steps: 100
fp16: true
learning_rate: 2.0e-05
local_rank: 0
loss_rate: 0.0
max_length: 256
max_samples: 0
max_steps: 100000
model_name: meta-llama/Llama-3.2-1B
output_dir: output_Llama3.2-1B/tp_Llama3.2-1B_precision-fp32_Num_Nodes-2_Data-winogrande_lr0_batch_size_8
patience: 5
seed: 1234
target_accuracy: 0.95
tensor_parallel_size: 2
weight_decay: 0.01
