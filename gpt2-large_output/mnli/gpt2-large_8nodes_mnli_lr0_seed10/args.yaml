batch_size: 16
dataset: mnli
epochs: 7
eval_steps: 20
fp16: true
ge_config: default
learning_rate: 2.0e-05
logging_steps: 10
loss_rate: 0.0
loss_type: ber
max_length: 256
max_samples: 0
model_name: meta-llama/Llama-3.2-1B
num_nodes: 8
num_unfrozen_layers: null
output_dir: gpt2-large_output/mnli
run_id: gpt2-large_8nodes_mnli_lr0_seed10
save_steps: 100
seed: 10
