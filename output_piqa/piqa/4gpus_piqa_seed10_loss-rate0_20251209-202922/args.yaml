batch_size: 8
dataset: piqa
det_config: default
epochs: 20
eval_steps: 20
fp16: true
ge_config: default
learning_rate: 1.0e-05
logging_steps: 10
loss_enable_ag: false
loss_enable_all: true
loss_enable_ar: false
loss_enable_rs: false
loss_rate: 0.0
loss_type: ber
max_length: 256
max_samples: 0
model_name: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
num_nodes: 2
num_unfrozen_layers: null
output_dir: output_piqa/piqa
run_id: 4gpus_piqa_seed10_loss-rate0_20251209-202922
save_steps: 100
seed: 10
