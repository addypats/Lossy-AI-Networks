batch_size: 128
dataset: piqa
epochs: 4
eval_steps: 20
fp16: false
ge_config: short_half_percent
learning_rate: 5.0e-06
logging_steps: 10
loss_rate: 0.001
loss_type: g-e
max_length: 256
max_samples: 0
model_name: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
num_nodes: 8
num_unfrozen_layers: null
output_dir: output_piqa/ge__output/piqa
run_id: ge__8nodes_piqa_lr_short_half_percent_seed10
save_steps: 100
seed: 10
