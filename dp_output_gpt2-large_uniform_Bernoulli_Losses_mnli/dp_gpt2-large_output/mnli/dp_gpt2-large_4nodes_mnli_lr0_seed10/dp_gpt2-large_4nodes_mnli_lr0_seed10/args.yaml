batch_size: 2
dataset: mnli
epochs: 7
eval_steps: 20
fp16: false
ge_config: default
learning_rate: 2.0e-05
logging_steps: 10
loss_rate: 0.0
loss_type: ber
max_length: 256
max_samples: 0
model_name: meta-llama/Llama-3.2-1B
num_nodes: 4
num_unfrozen_layers: null
output_dir: dp_output_gpt2-large_uniform_Bernoulli_Losses_mnli/dp_gpt2-large_output/mnli/dp_gpt2-large_4nodes_mnli_lr0_seed10
run_id: dp_gpt2-large_4nodes_mnli_lr0_seed10
save_steps: 100
seed: 10
