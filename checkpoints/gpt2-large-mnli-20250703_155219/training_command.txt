python -m torch.distributed.launch     --nproc_per_node=2     --nnodes=1     --node_rank=0     --master_addr=localhost     --master_port=12345     /home/ubuntu/Lossy-AI-Networks/Megatron-LM/pretrain_gpt.py     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --num-layers 36     --hidden-size 1280     --num-attention-heads 20     --seq-length 512     --max-position-embeddings 1024     --micro-batch-size 4     --global-batch-size 32     --lr 1e-5     --train-iters 10000     --lr-decay-iters 10000     --lr-decay-style cosine     --lr-warmup-fraction 0.1     --weight-decay 0.01     --clip-grad 1.0     --fp16     --vocab-file /home/ubuntu/Lossy-AI-Networks/data/gpt2-vocab.json     --merge-file /home/ubuntu/Lossy-AI-Networks/data/gpt2-merges.txt     --data-path /home/ubuntu/Lossy-AI-Networks/data/mnli_train.jsonl     --save /home/ubuntu/Lossy-AI-Networks/checkpoints/gpt2-large-mnli-20250703_155219     --load /home/ubuntu/Lossy-AI-Networks/checkpoints/gpt2-large-mnli-20250703_155219     --save-interval 100     --eval-interval 20     --eval-iters 10     --log-interval 10     --tensorboard-dir /home/ubuntu/Lossy-AI-Networks/checkpoints/gpt2-large-mnli-20250703_155219/tensorboard     --wandb-project megatron-gpt2-mnli-finetune     --seed 1234
