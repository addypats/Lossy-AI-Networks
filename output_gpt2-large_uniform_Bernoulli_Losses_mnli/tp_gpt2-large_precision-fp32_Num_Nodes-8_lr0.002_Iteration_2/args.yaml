batch_size: 8
dataset: mnli
eval_steps: 20
fp16: false
ge_config: default
learning_rate: 3.0e-05
local_rank: 0
loss_rate: 0.002
loss_type: ber
max_length: 128
max_samples: 0
max_steps: 100000
model_name: gpt2-large
output_dir: output_gpt2-large_uniform_Bernoulli_Losses_mnli/tp_gpt2-large_precision-fp32_Num_Nodes-8_lr0.002_Iteration_2
patience: 10
seed: 1234
target_accuracy: 0.75
tensor_parallel_size: 8
weight_decay: 0.01
